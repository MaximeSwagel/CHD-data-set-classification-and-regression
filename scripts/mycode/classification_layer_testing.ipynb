{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximeswagel/Library/Mobile Documents/com~apple~CloudDocs/Documents/EPFL/DTU/Courses/SecondSemester/MachineLearning/Assignment2/scripts\n",
      "\n",
      "Location of the south_african_heart_disease.xls file: /Users/maximeswagel/Library/Mobile Documents/com~apple~CloudDocs/Documents/EPFL/DTU/Courses/SecondSemester/MachineLearning/Assignment2/data/south_african_heart_disease.xls\n"
     ]
    }
   ],
   "source": [
    "from loading_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we want to normalize and transform our data.\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "normalised_X = np.copy(X)\n",
    "#transform\n",
    "normalised_X[:,6] = np.log(1 + X[:,6]) #add 1 because some alcohol values are 0\n",
    "#normalise\n",
    "normalised_X = zscore(normalised_X, axis = 0, ddof = 1)\n",
    "\n",
    "attributeNames_norm = np.copy(attributeNames)\n",
    "attributeNames_norm[6] = 'log-alc'\n",
    "attributeNames_norm = ['normalized ' + attribute for attribute in attributeNames_norm]\n",
    "\n",
    "#Or without the last binary data\n",
    "\n",
    "Y = np.copy(normalised_X[:,:-1])\n",
    "N_y, M_y = Y.shape\n",
    "\n",
    "attributeNames_y = np.copy(attributeNames_norm[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from dtuimldmtools import train_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#================OUTER LOOP 1================#\n",
      "#================INNER LOOP 1================#\n",
      "#================ANN Model 1================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.5080905\t0.00016760944\n",
      "\t\t2000\t0.4657387\t7.742113e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2836\t0.43919742\t2.7142522e-07\n",
      "#================ANN Model 2================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.46111357\t0.00025716602\n",
      "\t\t2000\t0.39118046\t0.00013224073\n",
      "\t\tFinal loss:\n",
      "\t\t2099\t0.38791028\t9.987634e-07\n",
      "#================INNER LOOP 2================#\n",
      "#================ANN Model 1================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.50056654\t0.00014405923\n",
      "\t\t2000\t0.4410043\t0.00013946216\n",
      "\t\tFinal loss:\n",
      "\t\t2740\t0.4115165\t5.0694484e-07\n",
      "#================ANN Model 2================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.48805848\t0.00018364382\n",
      "\t\t2000\t0.4455217\t4.46157e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2604\t0.4349876\t1.3702606e-07\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.5286668\t0.0005318743\n",
      "\t\t2000\t0.48033175\t1.0857808e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2067\t0.47971264\t9.940048e-07\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "       0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0])]\n",
      "#================OUTER LOOP 2================#\n",
      "#================INNER LOOP 1================#\n",
      "#================ANN Model 1================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.39685047\t0.00024182929\n",
      "\t\t2000\t0.32017154\t0.00021571836\n",
      "\t\tFinal loss:\n",
      "\t\t2013\t0.31959566\t2.7975034e-07\n",
      "#================ANN Model 2================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.43974006\t5.645138e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1818\t0.37708363\t6.3226935e-07\n",
      "#================INNER LOOP 2================#\n",
      "#================ANN Model 1================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.51849025\t0.00033602447\n",
      "\t\t2000\t0.41936025\t0.0001492877\n",
      "\t\tFinal loss:\n",
      "\t\t2919\t0.37973478\t4.7089142e-07\n",
      "#================ANN Model 2================#\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.48124626\t6.340962e-05\n",
      "\t\t2000\t0.45586914\t4.7852012e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2147\t0.4525071\t6.5860455e-08\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t0.517751\t0.0001496365\n",
      "\t\tFinal loss:\n",
      "\t\t1836\t0.49422428\t9.045173e-07\n",
      "[array([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "#We need to consider the possible lambdas and the number of hidden units that we want to consider in the inner loop. \n",
    "#For the baseline model there is no controlling parameter\n",
    "\n",
    "#First try\n",
    "n_hidden_units = [(2,2),(2,3)]\n",
    "# print(n_hidden_units)\n",
    "\n",
    "K_out = 2\n",
    "K_in = 2\n",
    "CV_out = model_selection.KFold(K_out,shuffle=True)\n",
    "CV_in = model_selection.KFold(K_in,shuffle=True)\n",
    "\n",
    "# For statistical evaluation : store outer fold predictions for the three models\n",
    "yhat = []\n",
    "y_true = []\n",
    "\n",
    "# For debugging : store inner fold predictions (for each outer fold and inner fold)\n",
    "ANN_full = {}\n",
    "\n",
    "#The Error for the best model of each type of model in each outer loop\n",
    "# Train_error = np.zeros((K_out,3))\n",
    "Test_error= np.zeros((K_out,2))\n",
    "best_h_index = np.int32(np.zeros(K_out))\n",
    "\n",
    "for k, (train_index,test_index) in enumerate(CV_out.split(normalised_X,y)):\n",
    "    print(f\"#================OUTER LOOP {k+1}================#\")\n",
    "    #to store the new predictions of the selected model at each outer fold (to then be concatenated in yhat)\n",
    "    dy = []\n",
    "\n",
    "    #the training tests for each fold of the outer loop\n",
    "    X_train = normalised_X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    X_test = normalised_X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    #Baseline model\n",
    "    baseline = DummyClassifier(strategy='most_frequent')\n",
    "    baseline.fit(X_train,y_train)\n",
    "    Test_error[k,1] = 1-baseline.score(X_test,y_test)\n",
    "\n",
    "    #INITIALIZE ERROR HANDLING\n",
    "    #Error for each model in each of the loops, overwritten at each outer loop \n",
    "    ANN_Inner_test_error= np.zeros((K_in, len(n_hidden_units)))\n",
    "\n",
    "    #average error of each model on each outter fold\n",
    "    ANN_Model_out_test_error = np.zeros((K_out, len(n_hidden_units)))\n",
    "\n",
    "    #we also need to store the sizes of the folds\n",
    "    inner_fold_validate_sizes = np.zeros(K_in)\n",
    "\n",
    "    #Inner Loop\n",
    "    for i, (Inner_train_index, Inner_test_index) in enumerate(CV_in.split(X_train,y_train)):\n",
    "        print(f\"#================INNER LOOP {i+1}================#\")\n",
    "        #initialize the training and validation sets\n",
    "        X_subtrain = X_train[Inner_train_index]\n",
    "        y_subtrain = y[Inner_train_index]\n",
    "        X_validate = X_train[Inner_test_index]\n",
    "        y_validate = y_train[Inner_test_index]\n",
    "\n",
    "        #store the size of the validation set\n",
    "        inner_fold_validate_sizes[i] = X_validate.shape[0]\n",
    "\n",
    "        #==========#\n",
    "        #ANN\n",
    "        #==========#\n",
    "\n",
    "        #We convert the training and test sets to torch tensors\n",
    "\n",
    "        X_subtrain = torch.tensor(X_subtrain, dtype=torch.float32)\n",
    "        y_subtrain = torch.tensor(y_subtrain, dtype=torch.float32).reshape(-1, 1)\n",
    "        X_validate = torch.tensor(X_validate, dtype=torch.float32)\n",
    "        y_validate = torch.tensor(y_validate, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "        #ANN cross validation loop\n",
    "        for j, (n1,n2) in enumerate(n_hidden_units):\n",
    "            print(f\"#================ANN Model {j+1}================#\")\n",
    "            # The lambda-syntax defines an anonymous function, which is used here to\n",
    "            # make it easy to make new networks within each cross validation fold\n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n1),  # M features to H hiden units\n",
    "                # 1st transfer function, either Tanh or ReLU:\n",
    "                torch.nn.Tanh(),  \n",
    "                # torch.nn.ReLU(),\n",
    "                torch.nn.Linear(n1, n2),    # 1st hidden layer to 2nd hidden layer (n2 units)\n",
    "                torch.nn.ReLU(),            # 2nd activation function\n",
    "                torch.nn.Linear(n2, 1),     # Output layer: n2 to 1 output neuron\n",
    "                torch.nn.Sigmoid(),  # final tranfer function\n",
    "            )\n",
    "            #Loss function (Binary Cross Entropy)\n",
    "            loss_fn = torch.nn.BCELoss()\n",
    "            # Train for a maximum of 10000 steps, or until convergence\n",
    "            max_iter = 10000\n",
    "        \n",
    "            net, final_loss, learning_curve = train_neural_net(\n",
    "            model, loss_fn, X=X_subtrain, y=y_subtrain, n_replicates=1, max_iter=max_iter\n",
    "            )\n",
    "            y_validate_est = net(X_validate).detach().numpy()\n",
    "            y_validate_pred = (y_validate_est > 0.5).astype(int)\n",
    "            y_validate_np = y_validate.numpy()\n",
    "            ANN_Inner_test_error[i, j] = np.sum(y_validate_pred != y_validate) / len(y_validate)\n",
    "            ANN_full[(i,j,k)] = { 'predictions' : y_validate_pred.squeeze(), 'ground_truth' : y_validate_np.squeeze()}\n",
    "\n",
    "    #Average Model Error calculation for ANN   \n",
    "    ANN_Model_out_test_error = np.sum(inner_fold_validate_sizes[:,None]*ANN_Inner_test_error, axis = 0)/X_train.shape[0]\n",
    "    best_h_index[k] = int(np.argmin(ANN_Model_out_test_error))\n",
    "\n",
    "    #Retrain the best ANN model\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "    # best_h_index = int(best_h_index)\n",
    "    n1 = n_hidden_units[best_h_index[k]][0]\n",
    "    n2 = n_hidden_units[best_h_index[k]][1]\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, n1),  # M features to H hiden units\n",
    "                # 1st transfer function, either Tanh or ReLU:\n",
    "                torch.nn.ReLU(),  \n",
    "                # torch.nn.ReLU(),\n",
    "                torch.nn.Linear(n1, n2),    # 1st hidden layer to 2nd hidden layer (n2 units)\n",
    "                torch.nn.ReLU(),            # 2nd activation function\n",
    "                torch.nn.Linear(n2, 1),     # Output layer: n2 to 1 output neuron\n",
    "                torch.nn.Sigmoid(),  # final tranfer function\n",
    "            )\n",
    "    #Loss function (Binary Cross Entropy)\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    # Train for a maximum of 10000 steps, or until convergence\n",
    "    max_iter = 10000\n",
    "        \n",
    "    net, final_loss, learning_curve = train_neural_net(\n",
    "        model, loss_fn, X=X_train, y=y_train, n_replicates=1, max_iter=max_iter\n",
    "    )\n",
    "    y_test_est = net(X_test).detach().numpy()\n",
    "    y_test_pred = (y_test_est > 0.5).astype(int)\n",
    "    y_test = y_test.numpy()\n",
    "    Test_error[k, 0] = np.sum(y_test_pred != y_test) / len(y_test)\n",
    "\n",
    "\n",
    "    #to store the predictions\n",
    "    dy.append(y_test_pred.squeeze())\n",
    "    print(dy)\n",
    "    dy = np.stack(dy, axis=1)\n",
    "    yhat.append(dy)\n",
    "    y_true.append(y_test)\n",
    "\n",
    "yhat = np.concatenate(yhat)\n",
    "y_true = np.concatenate(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I make some python checks for myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal hidden unit parameter index : [0 0]\n",
      "[[0.28138528 0.35497835]\n",
      " [0.3030303  0.33766234]]\n"
     ]
    }
   ],
   "source": [
    "# The lambda show that there should be no control parameter. \n",
    "# This means that the problem is inherently complicated and that there is not risk of overfitting with logistic regression.\n",
    "# Maybe we can try with the logit link instead of the sigmoid function ?\n",
    "print(f'optimal hidden unit parameter index : {best_h_index}')\n",
    "# print(f'Optimal number of hidden units : {n_hidden_units[best_h_index]}')\n",
    "print(Test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to test the error handling structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much worse or much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
